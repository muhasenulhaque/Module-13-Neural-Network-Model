{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ab6ba-e00d-4dee-b878-ef3c942f302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b28422-b4b3-4f20-a6f9-00be4adae6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the data to be used on a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7fa6d-4cc0-45dd-b200-d501adb9f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Read the `applicants_data.csv` file into a Pandas DataFrame. Review the DataFrame, looking for categorical variables that will need to be encoded, as well as columns that could eventually define your features and target variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f0df7-8a9d-4970-9c46-5c80835e257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the applicants_data.csv file from the Resources folder into a Pandas DataFrame\n",
    "applicant_data_df = pd.read_csv(Path(\"./Resources/applicants_data.csv\"))\n",
    "\n",
    "# Review the DataFrame\n",
    "applicant_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c57297-9de9-4116-923d-b8412863f57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the data types associated with the columns\n",
    "applicant_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549668ba-b8db-45c2-87ba-c3dc17b4ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Drop the “EIN” (Employer Identification Number) and “NAME” columns from the DataFrame, because they are not relevant to the binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967325b6-7df8-4a9b-86c4-397bb091337e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop the 'EIN' and 'NAME' columns from the DataFrame\n",
    "applicant_data_df = applicant_data_df.drop(columns =[\"EIN\",\"NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e89e8-e11d-4db3-8153-a3359e9aa9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the DataFrame\n",
    "applicant_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df0d3a-6a6f-4e6b-929b-b17098d90d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Encode the dataset’s categorical variables using `OneHotEncoder`, and then place the encoded variables into a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87adf675-69af-4d05-beba-d303b5e9066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the list of the columns with categorical variables\n",
    "\n",
    "list(applicant_data_df.dtypes[applicant_data_df.dtypes == \"object\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ad08d-4a7f-4baf-b1d4-e2f2606d32ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd320cfa-bca1-4a64-96e6-90234cc157f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of categorical variables \n",
    "categorical_variables = ['APPLICATION_TYPE',\n",
    " 'AFFILIATION',\n",
    " 'CLASSIFICATION',\n",
    " 'USE_CASE',\n",
    " 'ORGANIZATION',\n",
    " 'INCOME_AMT',\n",
    " 'SPECIAL_CONSIDERATIONS']\n",
    "\n",
    "# Display the categorical variables list\n",
    "categorical_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2706bb8f-29b3-4a10-a2ac-b6abb8355b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f48a5-f91a-4c29-b53d-9084174446ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorcal variables using OneHotEncoder\n",
    "encoded_data = enc.fit_transform(applicant_data_df[categorical_variables])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f240065-ba3a-400b-85be-84477afb45f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the encoded variables\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_data,\n",
    "    columns = enc.get_feature_names(categorical_variables)\n",
    ")\n",
    "\n",
    "# Review the DataFrame\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bea9f9-f301-4425-93a4-5e9286941e47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with the columnns containing numerical variables from the original dataset\n",
    "numerical_variables_df = applicant_data_df.drop(columns = categorical_variables)\n",
    "\n",
    "# Review the DataFrame\n",
    "numerical_variables_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e2f563-e2cd-4ecc-9037-217da2c9a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Add the original DataFrame’s numerical variables to the DataFrame containing the encoded variables.\n",
    "\n",
    "#### ***Note*** To complete this step, you will employ the Pandas `concat()` function that was introduced earlier in this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a946237-6566-47b6-8216-84d3babdc760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the numerical variables from the original DataFrame to the one-hot encoding DataFrame\n",
    "encoded_df = pd.concat(\n",
    "    [\n",
    "        numerical_variables_df,\n",
    "        encoded_df\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Reveiw the DataFrame\n",
    "encoded_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed18eb0-8f4b-4a89-b4f6-8ff5081e9bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930f894-5ae1-4bad-8766-fceab7f53d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 5: Using the preprocessed data, create the features (`X`) and target (`y`) datasets. The target dataset should be defined by the preprocessed DataFrame column “IS_SUCCESSFUL”. The remaining columns should define the features dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766a4b4-31ff-457a-887c-c8d831dd3dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target set y using the IS_SUCCESSFUL column\n",
    "y = encoded_df[\"IS_SUCCESSFUL\"]\n",
    "\n",
    "# Display a sample of y\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ab025-ba7b-4ffe-be4a-5464d2813cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features set X by selecting all columns but IS_SUCCESSFUL\n",
    "X = encoded_df.drop(columns=[\"IS_SUCCESSFUL\"])\n",
    "\n",
    "# Review the features DataFrame\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c382e8-4e15-4a68-bf7c-4c88ff2eb676",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 6: Split the features and target sets into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f8b71-7d1e-407e-80bc-8a26bf1f461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "# Assign the function a random_state equal to 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324f9f9-242e-411a-a5f3-c4b6b27e9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 7: Use scikit-learn's `StandardScaler` to scale the features data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21cd628-1eca-4b52-839c-cf935a07bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b9836-d204-4006-9116-d4e1ab0d8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile and Evaluate a Binary Classification Model Using a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0724d3-1b58-4acb-b5ef-c54ef762f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Create a deep neural network by assigning the number of input features, the number of layers, and the number of neurons on each layer using Tensorflow’s Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9523d-9f15-44d9-9b9a-3828b2397801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = len(X_train.iloc[0])\n",
    "\n",
    "# Review the number of features\n",
    "number_input_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83910095-79cc-421f-a8e7-9db466fe5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d82c45-ece3-4916-9f92-cc71051b41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1 =  (number_input_features + 1) // 2 \n",
    "\n",
    "# Review the number hidden nodes in the first layer\n",
    "hidden_nodes_layer1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b4166-1cf1-4c85-9314-92c9b31e70ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2 =  (hidden_nodes_layer1 + 1) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "hidden_nodes_layer2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36265ab5-d709-43cc-aeca-7cdcfea88c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Sequential model instance\n",
    "nn = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703817ba-ebfb-4c67-8598-1e77e8cabb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the first hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8ece7-19fa-4193-9388-b034e304aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the second hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba1cd5-0a68-42b3-b62f-6b5990b49c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn.add(Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1d2e2-e671-4f8a-905b-0f9653931425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Sequential model summary\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3297aa-035b-4f0f-890f-c787bdb2b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Compile and fit the model using the `binary_crossentropy` loss function, the `adam` optimizer, and the `accuracy` evaluation metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38541b-8ad2-4aec-b9d3-9e425233871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41ca020a-1acf-43a1-8e90-563e280a9a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5322 - accuracy: 0.7415\n",
      "Epoch 2/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5320 - accuracy: 0.7411\n",
      "Epoch 3/50\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5316 - accuracy: 0.7409\n",
      "Epoch 4/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5324 - accuracy: 0.7415\n",
      "Epoch 5/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5319 - accuracy: 0.7411\n",
      "Epoch 6/50\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5315 - accuracy: 0.7421\n",
      "Epoch 7/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5316 - accuracy: 0.7417\n",
      "Epoch 8/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5315 - accuracy: 0.7416\n",
      "Epoch 9/50\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5312 - accuracy: 0.7414\n",
      "Epoch 10/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5312 - accuracy: 0.7419\n",
      "Epoch 11/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5312 - accuracy: 0.7423\n",
      "Epoch 12/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5309 - accuracy: 0.7409\n",
      "Epoch 13/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5306 - accuracy: 0.7418\n",
      "Epoch 14/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5313 - accuracy: 0.7422\n",
      "Epoch 15/50\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5308 - accuracy: 0.7421\n",
      "Epoch 16/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5305 - accuracy: 0.7409\n",
      "Epoch 17/50\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5307 - accuracy: 0.7425\n",
      "Epoch 18/50\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5303 - accuracy: 0.7418\n",
      "Epoch 19/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5302 - accuracy: 0.7420\n",
      "Epoch 20/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5306 - accuracy: 0.7420\n",
      "Epoch 21/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5302 - accuracy: 0.7425\n",
      "Epoch 22/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5301 - accuracy: 0.7421\n",
      "Epoch 23/50\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5303 - accuracy: 0.7423\n",
      "Epoch 24/50\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5297 - accuracy: 0.7421\n",
      "Epoch 25/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5299 - accuracy: 0.7417\n",
      "Epoch 26/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5299 - accuracy: 0.7421\n",
      "Epoch 27/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5294 - accuracy: 0.7429\n",
      "Epoch 28/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5301 - accuracy: 0.7427\n",
      "Epoch 29/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5300 - accuracy: 0.7419\n",
      "Epoch 30/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5298 - accuracy: 0.7425\n",
      "Epoch 31/50\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5291 - accuracy: 0.7425\n",
      "Epoch 32/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5298 - accuracy: 0.7421\n",
      "Epoch 33/50\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5295 - accuracy: 0.7428\n",
      "Epoch 34/50\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5292 - accuracy: 0.7424\n",
      "Epoch 35/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5297 - accuracy: 0.7419\n",
      "Epoch 36/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5293 - accuracy: 0.7429\n",
      "Epoch 37/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5294 - accuracy: 0.7423\n",
      "Epoch 38/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5295 - accuracy: 0.7425\n",
      "Epoch 39/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5290 - accuracy: 0.7425\n",
      "Epoch 40/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5293 - accuracy: 0.7428\n",
      "Epoch 41/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5289 - accuracy: 0.7435\n",
      "Epoch 42/50\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5288 - accuracy: 0.7427\n",
      "Epoch 43/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5292 - accuracy: 0.7427\n",
      "Epoch 44/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5287 - accuracy: 0.7426\n",
      "Epoch 45/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5283 - accuracy: 0.7422\n",
      "Epoch 46/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5285 - accuracy: 0.7429\n",
      "Epoch 47/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5283 - accuracy: 0.7425\n",
      "Epoch 48/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5289 - accuracy: 0.7427\n",
      "Epoch 49/50\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5287 - accuracy: 0.7429\n",
      "Epoch 50/50\n",
      "804/804 [==============================] - 3s 3ms/step - loss: 0.5285 - accuracy: 0.7436\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0dab1bd-1423-46ed-8359-22213a1dd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Evaluate the model using the test data to determine the model’s loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9a2f484-7690-42da-9a4c-59a197bd9469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 - 0s - loss: 0.5575 - accuracy: 0.7291 - 499ms/epoch - 2ms/step\n",
      "Loss: 0.5575293302536011, Accuracy: 0.7290962338447571\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "deb7ae71-0cf9-4b84-9db7-15d0924c7f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Save and export your model to an HDF5 file, and name the file `AlphabetSoup.h5`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1b30eeb-9e82-47b9-8a4c-2b546b23131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model's file path\n",
    "file_path = \"AlphabetSoup.h5\"\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34fdbc61-5fc5-49f8-bed1-ac5af5dfd158",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimize the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2522c4a5-95b0-43fa-b9ed-e835afbe83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Define at least three new deep neural network models (resulting in the original plus 3 optimization attempts). With each, try to improve on your first model’s predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b5eb618-1696-4235-987c-762a99936f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternative Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8baf4bee-4d6d-4848-b643-8db7100f5cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = len(X_train.iloc[0])\n",
    "\n",
    "# Review the number of features\n",
    "number_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "129aa724-401c-4c5e-8d78-bfc8aeb2b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A1 = nn.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30f814aa-e389-4479-8b77-223f0fb1583c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A1 = 64\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "hidden_nodes_layer1_A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf47078e-7851-4ee3-89e8-f32e9e18998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Sequential model instance\n",
    "nn_A1 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ac435-6f8e-4daa-ad87-6ddb568e18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First hidden layer\n",
    "hidden_layer1_A1 = 64//4\n",
    "\n",
    "\n",
    "# Output layer\n",
    "number_output_layer_A1 =2\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_A1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cab51e-1d9d-47b9-b400-02fc8144e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model\n",
    "nn_A1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f98598-5536-4a7e-9eba-c40bfb338ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A1 = nn_A1.fit(X_train_scaled, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac815a8b-a70e-4c6d-959b-2f3c8a5aefc5",
   "metadata": {},
   "source": [
    "#### Alternative Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc534d6-321f-4337-852f-b0fea6334c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = len(X_train.iloc[0])\n",
    "\n",
    "# Review the number of features\n",
    "number_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e952b-038c-4038-b79b-291005482721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a3028d-4ced-4f25-a844-7571aea83b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A2 =74\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "hidden_nodes_layer1_A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466332e9-9eed-4f8e-abf4-3477eb2ecba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Sequential model instance\n",
    "nn_A2 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e77f920-1122-443e-bd9d-2f66064b183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First hidden layer\n",
    "hidden_layer1_A1 = 64//8\n",
    "\n",
    "# Output layer\n",
    "number_output_layer_A1 =3\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_A2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc560f9-baba-4a9a-9e26-411439006fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn_A2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290975af-c979-4767-9a34-abd7e474f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "fit_model_A2 = nn_A1.fit(X_train_scaled, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679b84b-e103-4b16-b642-44baeb8094f9",
   "metadata": {},
   "source": [
    "### Step 2: After finishing your models, display the accuracy scores achieved by each model, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a81ae8-1b3d-4a69-aadb-5a04a1ed9cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Model Results\")\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = # YOUR CODE HERE\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e370d-a1f6-495f-b6f1-c28735844c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Alternative Model 1 Results\")\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy =# YOUR CODE HERE\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7565c56-f636-4ef0-9865-3500ac7d00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Alternative Model 2 Results\")\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = # YOUR CODE HERE\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bab353-51a1-4ef0-bcbc-96b6c224ebf2",
   "metadata": {},
   "source": [
    "### Step 3: Save each of your alternative models as an HDF5 file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d5e3c-fbd4-401a-858b-0c8d14c0022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path for the first alternative model\n",
    "file_path = \"Alternative_Model.h5\"\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ca958-f093-45d3-afe9-3ddd0f559a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path for the second alternative model\n",
    "file_path = # YOUR CODE HERE\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcffc3d-1304-4a08-b104-08a29469315a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
